{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ4JAfmuPJC7"
      },
      "source": [
        "# Initializing the setup"
      ],
      "id": "fJ4JAfmuPJC7"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "82528ac9-5f34-4b07-bf0c-fe20202618a9"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "import pandas as pd\n",
        "import openai"
      ],
      "id": "82528ac9-5f34-4b07-bf0c-fe20202618a9"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b8jWzpVRL2jI"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import time"
      ],
      "id": "b8jWzpVRL2jI"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "67f69062-8bbb-4fe6-8e59-1e8e29a5601a"
      },
      "outputs": [],
      "source": [
        "openai.api_key = ### api key ###"
      ],
      "id": "67f69062-8bbb-4fe6-8e59-1e8e29a5601a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvDFvzpDPgzx"
      },
      "source": [
        "# Define annotation function"
      ],
      "id": "GvDFvzpDPgzx"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "899cZG1ZejOE"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json"
      ],
      "id": "899cZG1ZejOE"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Yeys7hS26ryi"
      },
      "outputs": [],
      "source": [
        "def generate_tweet(claim, task, model, temperature=1, max_tokens=2048):\n",
        "\n",
        "    # Set the prompt based on the task\n",
        "    if task in ['E', 'entailment', 'entail']:\n",
        "        instruction = \"Generate TWEET so that if TWEET is true, then CLAIM is also true. Be brief. Do not start a sentence with 'Just'.\"\n",
        "        prompt = f\"CLAIM: {claim}\\nTWEET:\"\n",
        "\n",
        "    elif task in ['C', 'contradiction', 'contradict']:\n",
        "        instruction = \"Generate TWEET so that if TWEET is true, CLAIM is false. Be brief. Do not start a sentence with 'Just'.\"\n",
        "        prompt = f\"CLAIM: {claim}\\nTWEET:\"\n",
        "\n",
        "    elif task in ['N', 'neutral']:\n",
        "        instruction = \"Generate TWEET so that even if TWEET is true, CLAIM cannot be said to be true or false. Be brief. Do not start a sentence with 'Just'. Use keywords from CLAIM.\"\n",
        "        prompt = f\"CLAIM: {claim}\\nTWEET:\"\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid task value.\")\n",
        "\n",
        "    if model[0:3] == 'gpt':\n",
        "        # Generate the response using OpenAI's API\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": instruction},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    elif model[0:5] == 'Llama' or model[0:5] == 'llama':\n",
        "\n",
        "        if model.find('70b') >= 0:\n",
        "            llama = \"meta-llama/Llama-2-70b-chat-hf\"\n",
        "        elif model.find('13b') >= 0:\n",
        "            llama = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "        elif model.find('7b') >= 0:\n",
        "            llama = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "        else:\n",
        "            print('No model size found. Defaults to Llama-2-70b-chat-hf')\n",
        "\n",
        "        TOKEN = ### HF API KEY ###\n",
        "        tokens=2048\n",
        "        input = f\"\"\"<s>[INST] <<SYS>> {instruction} <</SYS>> {prompt} [/INST]\"\"\"\n",
        "\n",
        "        url = f'https://api-inference.huggingface.co/models/{llama}'\n",
        "        headers = {\n",
        "                \"Content-type\": \"application/json\",\n",
        "                \"Authorization\": f'Bearer {TOKEN}',\n",
        "            }\n",
        "        body = {\n",
        "                \"inputs\": input,\n",
        "\n",
        "                \"parameters\": {\"temperature\": 1,\n",
        "                              \"max_new_tokens\": tokens,\n",
        "                              \"return_full_text\": False},\n",
        "            }\n",
        "\n",
        "        response = requests.post(url, headers=headers, data=json.dumps(body))\n",
        "        return response.json()[0]['generated_text'].strip().split('\\n')[0]"
      ],
      "id": "Yeys7hS26ryi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obCrCUb_QK6_"
      },
      "source": [
        "# Testing with prompt"
      ],
      "id": "obCrCUb_QK6_"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0nWPn0dZOcen"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "claim = \"Vaccininated people emit Bluetooth signals.\"\n",
        "\n",
        "entail = generate_tweet(claim, 'E', 'llama-70b')\n",
        "print(entail)\n",
        "\n",
        "contradict = generate_tweet(claim, 'C', 'llama-70b')\n",
        "print(contradict)\n",
        "\n",
        "neutral = generate_tweet(claim, 'N', 'llama-70b')\n",
        "print(neutral)\n",
        "\n",
        "end_time = time.time()\n",
        "runtime = end_time - start_time\n",
        "print(runtime)"
      ],
      "id": "0nWPn0dZOcen"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a4_RO3GBPG7"
      },
      "source": [
        "# Open dataframe"
      ],
      "id": "0a4_RO3GBPG7"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lFKWsaCqBhGM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "lFKWsaCqBhGM"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c0r1ioxLBk18"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/FACT-GPT dataset.csv')"
      ],
      "id": "c0r1ioxLBk18"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3r7_tN9Sigv"
      },
      "source": [
        "# Annotation loop"
      ],
      "id": "a3r7_tN9Sigv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZKP2hc2kKU8"
      },
      "source": [
        "### gpt-4 generation"
      ],
      "id": "6ZKP2hc2kKU8"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4f0547ae-7164-4391-86e3-e4401a3ad584"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for i, row in df.iterrows():\n",
        "\n",
        "    claim = row['claim']\n",
        "    retry = 0\n",
        "\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            if pd.isnull(row['generated_entail_tweet_gpt-4']):\n",
        "                df.at[i, 'generated_entail_tweet_gpt-4'] = generate_tweet(claim, 'E', 'llama-70b')\n",
        "\n",
        "            if pd.isnull(row['generated_contradict_tweet_gpt-4']):\n",
        "                df.at[i, 'generated_contradict_tweet_gpt-4'] = generate_tweet(claim, 'C', 'llama-70b')\n",
        "\n",
        "            if pd.isnull(row['generated_neutral_tweet_gpt-4']):\n",
        "                df.at[i, 'generated_neutral_tweet_gpt-4'] = generate_tweet(claim, 'N', 'llama-70b')\n",
        "\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(10)\n",
        "            retry += 1\n",
        "            if retry >= 50:\n",
        "                break\n",
        "\n",
        "    df.to_csv('/content/drive/MyDrive/FACT-GPT dataset.csv')\n",
        "\n",
        "    end_time = time.time()\n",
        "    runtime = end_time - start_time\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    print(f\"Iteration: {i+1}, Runtime: {runtime} seconds\")"
      ],
      "id": "4f0547ae-7164-4391-86e3-e4401a3ad584"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unassign runtime"
      ],
      "metadata": {
        "id": "GwN77APX2klX"
      },
      "id": "GwN77APX2klX"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fo1-NfahZwZB"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "id": "fo1-NfahZwZB"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6ZKP2hc2kKU8",
        "z1ucgMeyjLRB"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}