{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fJ4JAfmuPJC7",
   "metadata": {
    "id": "fJ4JAfmuPJC7"
   },
   "source": [
    "# Initializing the setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82528ac9-5f34-4b07-bf0c-fe20202618a9",
   "metadata": {
    "id": "82528ac9-5f34-4b07-bf0c-fe20202618a9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8jWzpVRL2jI",
   "metadata": {
    "id": "b8jWzpVRL2jI"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f69062-8bbb-4fe6-8e59-1e8e29a5601a",
   "metadata": {
    "id": "67f69062-8bbb-4fe6-8e59-1e8e29a5601a"
   },
   "outputs": [],
   "source": [
    "with open(\"OPENAI_API_KEY.txt\", \"r\") as file:\n",
    "    openai.api_key = file.read()\n",
    "\n",
    "with open(\"HUGGINGFACE_API_KEY.txt\", \"r\") as file:\n",
    "    TOKEN = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GvDFvzpDPgzx",
   "metadata": {
    "id": "GvDFvzpDPgzx"
   },
   "source": [
    "# Define annotation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899cZG1ZejOE",
   "metadata": {
    "id": "899cZG1ZejOE"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yeys7hS26ryi",
   "metadata": {
    "id": "Yeys7hS26ryi"
   },
   "outputs": [],
   "source": [
    "def generate_tweet(claim, task, model, temperature=1, max_tokens=2048):\n",
    "\n",
    "    # Set the prompt based on the task\n",
    "    if task in ['E', 'entailment', 'entail']:\n",
    "        instruction = \"Generate TWEET so that if TWEET is true, then CLAIM is also true. Be brief. Do not start a sentence with 'Just'.\"\n",
    "        prompt = f\"CLAIM: {claim}\\nTWEET:\"\n",
    "\n",
    "    elif task in ['C', 'contradiction', 'contradict']:\n",
    "        instruction = \"Generate TWEET so that if TWEET is true, CLAIM is false. Be brief. Do not start a sentence with 'Just'.\"\n",
    "        prompt = f\"CLAIM: {claim}\\nTWEET:\"\n",
    "\n",
    "    elif task in ['N', 'neutral']:\n",
    "        instruction = \"Generate TWEET so that even if TWEET is true, CLAIM cannot be said to be true or false. Be brief. Do not start a sentence with 'Just'. Use keywords from CLAIM.\"\n",
    "        prompt = f\"CLAIM: {claim}\\nTWEET:\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task value.\")\n",
    "\n",
    "    if model[0:3] == 'gpt':\n",
    "        # Generate the response using OpenAI's API\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    elif model[0:5] == 'Llama' or model[0:5] == 'llama':\n",
    "\n",
    "        if model.find('70b') >= 0:\n",
    "            llama = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "        elif model.find('13b') >= 0:\n",
    "            llama = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "        elif model.find('7b') >= 0:\n",
    "            llama = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "        else:\n",
    "            print('No model size found. Defaults to Llama-2-70b-chat-hf')\n",
    "\n",
    "        tokens=2048\n",
    "        input = f\"\"\"<s>[INST] <<SYS>> {instruction} <</SYS>> {prompt} [/INST]\"\"\"\n",
    "\n",
    "        url = f'https://api-inference.huggingface.co/models/{llama}'\n",
    "        headers = {\n",
    "                \"Content-type\": \"application/json\",\n",
    "                \"Authorization\": f'Bearer {TOKEN}',\n",
    "            }\n",
    "        body = {\n",
    "                \"inputs\": input,\n",
    "\n",
    "                \"parameters\": {\"temperature\": 1,\n",
    "                              \"max_new_tokens\": tokens,\n",
    "                              \"return_full_text\": False},\n",
    "            }\n",
    "\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(body))\n",
    "        return response.json()[0]['generated_text'].strip().split('\\n')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obCrCUb_QK6_",
   "metadata": {
    "id": "obCrCUb_QK6_"
   },
   "source": [
    "# Testing with prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0nWPn0dZOcen",
   "metadata": {
    "id": "0nWPn0dZOcen"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "claim = \"Vaccininated people emit Bluetooth signals.\"\n",
    "\n",
    "entail = generate_tweet(claim, 'E', 'gpt-4')\n",
    "print(entail)\n",
    "\n",
    "contradict = generate_tweet(claim, 'C', 'gpt-4')\n",
    "print(contradict)\n",
    "\n",
    "neutral = generate_tweet(claim, 'N', 'gpt-4')\n",
    "print(neutral)\n",
    "\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1295489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "claim = \"Vaccininated people emit Bluetooth signals.\"\n",
    "\n",
    "entail = generate_tweet(claim, 'E', 'llama-2-70b')\n",
    "print(entail)\n",
    "\n",
    "contradict = generate_tweet(claim, 'C', 'llama-2-70b')\n",
    "print(contradict)\n",
    "\n",
    "neutral = generate_tweet(claim, 'N', 'llama-2-70b')\n",
    "print(neutral)\n",
    "\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4_RO3GBPG7",
   "metadata": {
    "id": "0a4_RO3GBPG7"
   },
   "source": [
    "# Open dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0r1ioxLBk18",
   "metadata": {
    "id": "c0r1ioxLBk18"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('FACT-GPT dataset.csv', index_col=0)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3r7_tN9Sigv",
   "metadata": {
    "id": "a3r7_tN9Sigv"
   },
   "source": [
    "# Annotation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ZKP2hc2kKU8",
   "metadata": {
    "id": "6ZKP2hc2kKU8"
   },
   "source": [
    "### gpt-4 generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0547ae-7164-4391-86e3-e4401a3ad584",
   "metadata": {
    "id": "4f0547ae-7164-4391-86e3-e4401a3ad584"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    claim = row['claim']\n",
    "    retry = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        try:\n",
    "            if pd.isnull(row['generated_entail_tweet_gpt-4']):\n",
    "                df.at[i, 'generated_entail_tweet_gpt-4'] = generate_tweet(claim, 'E', 'gpt-4')\n",
    "\n",
    "            if pd.isnull(row['generated_contradict_tweet_gpt-4']):\n",
    "                df.at[i, 'generated_contradict_tweet_gpt-4'] = generate_tweet(claim, 'C', 'gpt-4')\n",
    "\n",
    "            if pd.isnull(row['generated_neutral_tweet_gpt-4']):\n",
    "                df.at[i, 'generated_neutral_tweet_gpt-4'] = generate_tweet(claim, 'N', 'gpt-4')\n",
    "\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(10)\n",
    "            retry += 1\n",
    "            if retry >= 50:\n",
    "                break\n",
    "\n",
    "    df.to_csv('FACT-GPT dataset.csv')\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    print(f\"Iteration: {i+1}, Runtime: {runtime} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GwN77APX2klX",
   "metadata": {
    "id": "GwN77APX2klX"
   },
   "source": [
    "# Unassign runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fo1-NfahZwZB",
   "metadata": {
    "id": "fo1-NfahZwZB"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6ZKP2hc2kKU8",
    "z1ucgMeyjLRB"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
