{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp-wnw44P6I6"
   },
   "source": [
    "# Open dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z5LG-pqBFSpl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "igVNpufMbUK2"
   },
   "outputs": [],
   "source": [
    "original_df = pd.read_csv('FACT-GPT dataset.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "L9vwftzpN7HI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['platform', 'claim_number', 'claim', 'date_claimed', 'date_checked',\n",
       "       'verdict', 'tweet_id', 'tweet_date', 'score', 'Mturk_1', 'Mturk_2',\n",
       "       'Mturk_3', 'Mturk_4', 'Mturk_5', 'generated_entail_tweet_gpt-4',\n",
       "       'generated_contradict_tweet_gpt-4', 'generated_neutral_tweet_gpt-4',\n",
       "       'annotation_only_gpt-4', 'annotation_only_gpt-3_5',\n",
       "       'annotation_only_70b', 'annotation_only_13b', 'annotation_only_7b',\n",
       "       'gpt-3_5_finetuned_on_gpt_4', 'gpt-3_5_finetuned_on_gpt_3_5',\n",
       "       'gpt-3_5_finetuned_on_70b', '13b_finetuned_on_gpt_4',\n",
       "       '13b_finetuned_on_gpt_3_5', '13b_finetuned_on_70b',\n",
       "       '7b_finetuned_on_gpt_4', '7b_finetuned_on_gpt_3_5',\n",
       "       '7b_finetuned_on_70b'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ou82w2xFxgLv"
   },
   "outputs": [],
   "source": [
    "columns_eval = ['annotation_only_gpt-4',\n",
    " 'annotation_only_gpt-3_5',\n",
    " 'annotation_only_13b',\n",
    " 'annotation_only_7b',\n",
    " 'gpt-3_5_finetuned_on_gpt_4',\n",
    " 'gpt-3_5_finetuned_on_gpt_3_5',\n",
    " 'gpt-3_5_finetuned_on_70b',\n",
    " '13b_finetuned_on_gpt_4',\n",
    " '13b_finetuned_on_gpt_3_5',\n",
    " '13b_finetuned_on_70b',\n",
    " '7b_finetuned_on_gpt_4',\n",
    " '7b_finetuned_on_gpt_3_5',\n",
    " '7b_finetuned_on_70b'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC4owfnjFsV2"
   },
   "source": [
    "## Preprocess predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4nhkBpVBUXmt"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_all_annotation(s):\n",
    "    # Convert the string to uppercase for case-insensitive matching.\n",
    "    s = str(s).upper()\n",
    "    tokens = re.findall(r'\\w+', s)\n",
    "\n",
    "    # Map for valid tokens that simplifies various forms to their base form.\n",
    "    valid_tokens_map = {\n",
    "        'ENTAIL': 'ENTAILMENT',\n",
    "        'ENTAILS': 'ENTAILMENT',\n",
    "        'ENTAILING': 'ENTAILMENT',\n",
    "        'ENTAILMENT': 'ENTAILMENT',\n",
    "\n",
    "        'CONTRADICT': 'CONTRADICTION',\n",
    "        'CONTRADICTS': 'CONTRADICTION',\n",
    "        'CONTRADICTING': 'CONTRADICTION',\n",
    "        'CONTRADICTION': 'CONTRADICTION',\n",
    "\n",
    "        'NEUTRAL': 'NEUTRAL',\n",
    "    }\n",
    "\n",
    "    valid_tokens = set([\n",
    "        'ENTAILMENT', 'NEUTRAL', 'CONTRADICTION',\n",
    "    ])\n",
    "\n",
    "    # We map each token to its valid form if present in the map, otherwise keep the token as is\n",
    "    annotate_list = [valid_tokens_map.get(token, token) for token in tokens if token in valid_tokens_map.keys()]\n",
    "\n",
    "    # Return the first valid annotation found, or None if none are found.\n",
    "    if len(annotate_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return annotate_list[0]\n",
    "\n",
    "# Apply the modified function:\n",
    "original_df[columns_eval] = original_df[columns_eval].applymap(find_all_annotation)\n",
    "\n",
    "# This line is meant to demonstrate how to find indices where the column values are None, which might not directly relate to fixing the function.\n",
    "none_indices = {col: original_df[original_df[col].isna()].index.tolist() for col in columns_eval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GQOCCeyMIYn6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_only_gpt-4</th>\n",
       "      <th>annotation_only_gpt-3_5</th>\n",
       "      <th>annotation_only_13b</th>\n",
       "      <th>annotation_only_7b</th>\n",
       "      <th>gpt-3_5_finetuned_on_gpt_4</th>\n",
       "      <th>gpt-3_5_finetuned_on_gpt_3_5</th>\n",
       "      <th>gpt-3_5_finetuned_on_70b</th>\n",
       "      <th>13b_finetuned_on_gpt_4</th>\n",
       "      <th>13b_finetuned_on_gpt_3_5</th>\n",
       "      <th>13b_finetuned_on_70b</th>\n",
       "      <th>7b_finetuned_on_gpt_4</th>\n",
       "      <th>7b_finetuned_on_gpt_3_5</th>\n",
       "      <th>7b_finetuned_on_70b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation_only_gpt-4 annotation_only_gpt-3_5 annotation_only_13b  \\\n",
       "0               NEUTRAL           CONTRADICTION       CONTRADICTION   \n",
       "1               NEUTRAL              ENTAILMENT       CONTRADICTION   \n",
       "2               NEUTRAL                 NEUTRAL       CONTRADICTION   \n",
       "3         CONTRADICTION                 NEUTRAL       CONTRADICTION   \n",
       "4               NEUTRAL                 NEUTRAL       CONTRADICTION   \n",
       "\n",
       "  annotation_only_7b gpt-3_5_finetuned_on_gpt_4 gpt-3_5_finetuned_on_gpt_3_5  \\\n",
       "0      CONTRADICTION                 ENTAILMENT                   ENTAILMENT   \n",
       "1         ENTAILMENT                 ENTAILMENT                   ENTAILMENT   \n",
       "2      CONTRADICTION              CONTRADICTION                CONTRADICTION   \n",
       "3      CONTRADICTION                    NEUTRAL                   ENTAILMENT   \n",
       "4      CONTRADICTION                 ENTAILMENT                   ENTAILMENT   \n",
       "\n",
       "  gpt-3_5_finetuned_on_70b 13b_finetuned_on_gpt_4 13b_finetuned_on_gpt_3_5  \\\n",
       "0               ENTAILMENT             ENTAILMENT               ENTAILMENT   \n",
       "1               ENTAILMENT                NEUTRAL                  NEUTRAL   \n",
       "2            CONTRADICTION                NEUTRAL            CONTRADICTION   \n",
       "3                  NEUTRAL                NEUTRAL                  NEUTRAL   \n",
       "4                  NEUTRAL             ENTAILMENT                  NEUTRAL   \n",
       "\n",
       "  13b_finetuned_on_70b 7b_finetuned_on_gpt_4 7b_finetuned_on_gpt_3_5  \\\n",
       "0           ENTAILMENT            ENTAILMENT              ENTAILMENT   \n",
       "1              NEUTRAL            ENTAILMENT              ENTAILMENT   \n",
       "2        CONTRADICTION         CONTRADICTION              ENTAILMENT   \n",
       "3              NEUTRAL               NEUTRAL              ENTAILMENT   \n",
       "4              NEUTRAL            ENTAILMENT              ENTAILMENT   \n",
       "\n",
       "  7b_finetuned_on_70b  \n",
       "0          ENTAILMENT  \n",
       "1          ENTAILMENT  \n",
       "2             NEUTRAL  \n",
       "3          ENTAILMENT  \n",
       "4          ENTAILMENT  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df[columns_eval].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "URqEbIkShak7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation_only_gpt-4: 0 rows are not one word\n",
      "annotation_only_gpt-3_5: 0 rows are not one word\n",
      "annotation_only_13b: 2 rows are not one word\n",
      "annotation_only_7b: 0 rows are not one word\n",
      "gpt-3_5_finetuned_on_gpt_4: 0 rows are not one word\n",
      "gpt-3_5_finetuned_on_gpt_3_5: 0 rows are not one word\n",
      "gpt-3_5_finetuned_on_70b: 0 rows are not one word\n",
      "13b_finetuned_on_gpt_4: 0 rows are not one word\n",
      "13b_finetuned_on_gpt_3_5: 0 rows are not one word\n",
      "13b_finetuned_on_70b: 1 rows are not one word\n",
      "7b_finetuned_on_gpt_4: 0 rows are not one word\n",
      "7b_finetuned_on_gpt_3_5: 0 rows are not one word\n",
      "7b_finetuned_on_70b: 0 rows are not one word\n"
     ]
    }
   ],
   "source": [
    "# Function to count rows that are not one word\n",
    "def count_non_single_word_rows(column):\n",
    "    return sum(original_df[column].apply(lambda x: len(str(x).split()) != 1 or x is None))\n",
    "\n",
    "counts = {column: count_non_single_word_rows(column) for column in original_df.columns if column in columns_eval}\n",
    "\n",
    "# Print the results\n",
    "for column, count in counts.items():\n",
    "    print(f\"{column}: {count} rows are not one word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfzcVk2VwPqO"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HigdSgYWFYJE"
   },
   "source": [
    "## Set ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TgH7zLW70rGk"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Columns for 'Mturk'\n",
    "mturk_columns = ['Mturk_1', 'Mturk_2', 'Mturk_3', 'Mturk_4', 'Mturk_5']\n",
    "\n",
    "# Function to calculate the majority vote, considering ties\n",
    "def majority_vote(row):\n",
    "    counter = Counter(row)\n",
    "    max_count = max(counter.values())\n",
    "    return [k for k, v in counter.items() if v == max_count]\n",
    "\n",
    "# Calculate the majority vote for 'Mturk~'\n",
    "original_df['Majority_Mturk'] = original_df[mturk_columns].apply(majority_vote, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CpEJUzBVSFoF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ENTAILMENT]                   647\n",
       "[NEUTRAL]                      433\n",
       "[CONTRADICTION]                 90\n",
       "[NEUTRAL, ENTAILMENT]           17\n",
       "[ENTAILMENT, NEUTRAL]           17\n",
       "[CONTRADICTION, NEUTRAL]         8\n",
       "[CONTRADICTION, ENTAILMENT]      5\n",
       "[ENTAILMENT, CONTRADICTION]      5\n",
       "[NEUTRAL, CONTRADICTION]         3\n",
       "Name: Majority_Mturk, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df['Majority_Mturk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FQpCOaOHyYwV"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# # Function to randomly tie-break and aggregate based on the given scheme\n",
    "# def random_tie_break(val1_list):\n",
    "#     val1 = np.random.choice(val1_list, 1)[0] if isinstance(val1_list, list) else val1_list\n",
    "#     label_set = {'ENTAILMENT', 'NEUTRAL', 'CONTRADICTION'}\n",
    "\n",
    "#     if val1 == 'ENTAILMENT':\n",
    "#         return 'ENTAILMENT'\n",
    "#     elif val1 == 'CONTRADICTION':\n",
    "#         return 'CONTRADICTION'\n",
    "#     elif val1 not in label_set:\n",
    "#         return None\n",
    "#     else:\n",
    "#         return 'NEUTRAL'\n",
    "\n",
    "# # Set the seed for reproducibility\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # Convert the relevant columns to NumPy arrays for faster computation\n",
    "# majority_mturk = original_df['Majority_Mturk'].to_numpy()\n",
    "\n",
    "# # Initialize a list to store the randomly tie-broken and aggregated lists\n",
    "# random_aggregated_mturks = []\n",
    "\n",
    "# # Create 1000 random tie-broken and aggregated lists\n",
    "# for i in range(1000):\n",
    "#     random_aggregated_mturk = [random_tie_break(val1) for val1 in majority_mturk]\n",
    "#     random_aggregated_mturks.append(random_aggregated_mturk)\n",
    "\n",
    "\n",
    "# # Pickle the random_aggregated_mturks list\n",
    "# pickle_file_path = 'FACT-GPT eval tiebreak.pkl'\n",
    "# with open(pickle_file_path, 'wb') as f:\n",
    "#     pickle.dump(random_aggregated_mturks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AOBzMILGRyrl"
   },
   "outputs": [],
   "source": [
    "# Load the pickled random_aggregated_mturks list\n",
    "pickle_file_path = 'FACT-GPT eval tiebreak.pkl'\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    random_aggregated_mturks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZMsNamoL4GlB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ENTAILMENT': 0.5460334693877551,\n",
       "  'CONTRADICTION': 0.08204244897959184,\n",
       "  'NEUTRAL': 0.37192408163265306},\n",
       " {'ENTAILMENT': 668.891, 'CONTRADICTION': 100.502, 'NEUTRAL': 455.607})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a Counter to store the frequency of each class across all numpies\n",
    "overall_counter = Counter()\n",
    "\n",
    "# Count class frequencies across the 1,000 randomly aggregated lists\n",
    "for random_aggregated_mturk in random_aggregated_mturks:\n",
    "    counter = Counter(random_aggregated_mturk)\n",
    "    overall_counter += counter\n",
    "\n",
    "# Calculate the average frequency of each class\n",
    "total_count = sum(overall_counter.values())\n",
    "average_ratio = {k: v / total_count for k, v in overall_counter.items()}\n",
    "average_frequency = {k: v / 1000 for k, v in overall_counter.items()}\n",
    "average_ratio, average_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUZdWjyiFdcC"
   },
   "source": [
    "## Evaluate over 1,000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "20PlCWEF0vCx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, accuracy_score, f1_score\n",
    "from itertools import product\n",
    "\n",
    "def evaluate_aggregated_mturks(df, column_name, sample=1000):\n",
    "    # Extract the relevant columns\n",
    "    column = df[column_name].to_numpy()\n",
    "\n",
    "    # Initialize lists to store precision, recall, and accuracy\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    mic_precision_list = []\n",
    "    mic_recall_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    # Initialize a counter for None values\n",
    "    none_counter = 0\n",
    "\n",
    "    # Initialize a list to store the indices of None values\n",
    "    none_indices = []\n",
    "\n",
    "    # Generate 1000 random tie-broken and aggregated numpy arrays and evaluate metrics\n",
    "    for i in range(sample):\n",
    "        aggregated_col = []\n",
    "        for idx, val1 in enumerate(column):\n",
    "            if val1 is None:\n",
    "                none_counter += 1\n",
    "                none_indices.append(idx)\n",
    "                aggregated_col.append(None)\n",
    "                continue\n",
    "            aggregated_col.append(random_tie_break(val1))\n",
    "\n",
    "        aggregated_col = np.array(aggregated_col)\n",
    "\n",
    "        # Filter out None values for the metric calculations\n",
    "        valid_indices = aggregated_col != None\n",
    "        filtered_aggregated_col = aggregated_col[valid_indices]\n",
    "        filtered_reference = np.array([random_aggregated_mturk[i] for i in range(len(random_aggregated_mturk)) if valid_indices[i]])\n",
    "\n",
    "        # Calculate and store precision, recall, and accuracy\n",
    "        precision_list.append(precision_score(filtered_reference, filtered_aggregated_col, average='macro', zero_division=0))\n",
    "        recall_list.append(recall_score(filtered_reference, filtered_aggregated_col, average='macro', zero_division=0))\n",
    "        mic_precision_list.append(precision_score(filtered_reference, filtered_aggregated_col, average='micro', zero_division=0))\n",
    "        mic_recall_list.append(recall_score(filtered_reference, filtered_aggregated_col, average='micro', zero_division=0))\n",
    "        accuracy_list.append(accuracy_score(filtered_reference, filtered_aggregated_col))\n",
    "\n",
    "        # Calculate precision for each label\n",
    "        con_precision_list = precision_score(filtered_reference, filtered_aggregated_col, average=None, zero_division=0)[0]\n",
    "        ent_precision_list = precision_score(filtered_reference, filtered_aggregated_col, average=None, zero_division=0)[1]\n",
    "        neu_precision_list = precision_score(filtered_reference, filtered_aggregated_col, average=None, zero_division=0)[2]\n",
    "\n",
    "        # Calculate recall for each label\n",
    "        con_recall_list = recall_score(filtered_reference, filtered_aggregated_col, average=None, zero_division=0)[0]\n",
    "        ent_recall_list = recall_score(filtered_reference, filtered_aggregated_col, average=None, zero_division=0)[1]\n",
    "        neu_recall_list = recall_score(filtered_reference, filtered_aggregated_col, average=None, zero_division=0)[2]\n",
    "\n",
    "        # Calculate f1 for each label\n",
    "        con_f1_list = f1_score(filtered_reference, filtered_aggregated_col, average=None)[0]\n",
    "        ent_f1_list = f1_score(filtered_reference, filtered_aggregated_col, average=None)[1]\n",
    "        neu_f1_list = f1_score(filtered_reference, filtered_aggregated_col, average=None)[2]\n",
    "\n",
    "\n",
    "    avg_precision = np.mean(precision_list)\n",
    "    avg_recall = np.mean(recall_list)\n",
    "    avg_mic_precision = np.mean(mic_precision_list)\n",
    "    avg_mic_recall = np.mean(mic_recall_list)\n",
    "    avg_accuracy = np.mean(accuracy_list)\n",
    "\n",
    "    ent_precision = np.mean(ent_precision_list)\n",
    "    neu_precision = np.mean(neu_precision_list)\n",
    "    con_precision = np.mean(con_precision_list)\n",
    "\n",
    "    ent_recall = np.mean(ent_recall_list)\n",
    "    neu_recall = np.mean(neu_recall_list)\n",
    "    con_recall = np.mean(con_recall_list)\n",
    "\n",
    "    ent_f1 = np.mean(ent_f1_list)\n",
    "    neu_f1 = np.mean(neu_f1_list)\n",
    "    con_f1 = np.mean(con_f1_list)\n",
    "\n",
    "\n",
    "    return f\"\"\"Overall performance\n",
    "Macro precision: {avg_precision}\n",
    "Macro recall: {avg_recall}\n",
    "Micro precision: {avg_mic_precision}\n",
    "Micro recall: {avg_mic_recall}\n",
    "Accuracy: {avg_accuracy}\n",
    "\n",
    "Label-by-label (order: E-N-C)\n",
    "Precision: {ent_precision}, {neu_precision}, {con_precision}\n",
    "Recall: {ent_recall}, {neu_recall}, {con_recall}\n",
    "F1-score: {ent_f1}, {neu_f1}, {con_f1}\n",
    "\n",
    "None: {none_counter}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oi8na42XIM46",
    "outputId": "64b00bbc-834e-47d1-f409-c690cadef619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation_only_gpt-4\n",
      "Overall performance\n",
      "Macro precision: 0.6354674452014268\n",
      "Macro recall: 0.6966010606309113\n",
      "Micro precision: 0.6261224489795918\n",
      "Micro recall: 0.6261224489795918\n",
      "Accuracy: 0.6261224489795918\n",
      "\n",
      "Label-by-label (order: E-N-C)\n",
      "Precision: 0.9804560260586319, 0.5443037974683544, 0.38164251207729466\n",
      "Recall: 0.44925373134328356, 0.8505494505494505, 0.79\n",
      "F1-score: 0.616171954964176, 0.6638078902229846, 0.5146579804560261\n",
      "\n",
      "None: 0\n",
      "\n",
      "annotation_only_gpt-3_5\n",
      "Overall performance\n",
      "Macro precision: 0.5622234149134105\n",
      "Macro recall: 0.6093395659067301\n",
      "Micro precision: 0.5844897959183674\n",
      "Micro recall: 0.5844897959183674\n",
      "Accuracy: 0.5844897959183674\n",
      "\n",
      "Label-by-label (order: E-N-C)\n",
      "Precision: 0.8588235294117647, 0.5328467153284672, 0.295\n",
      "Recall: 0.43582089552238806, 0.8021978021978022, 0.59\n",
      "F1-score: 0.5782178217821782, 0.6403508771929826, 0.3933333333333333\n",
      "\n",
      "None: 0\n",
      "\n",
      "annotation_only_13b\n",
      "Overall performance\n",
      "Macro precision: 0.5098200561974132\n",
      "Macro recall: 0.4650455229371904\n",
      "Micro precision: 0.30171708912510214\n",
      "Micro recall: 0.30171708912510214\n",
      "Accuracy: 0.30171708912510214\n",
      "\n",
      "Label-by-label (order: E-N-C)\n",
      "Precision: 0.8876404494382022, 0.5327510917030568, 0.1090686274509804\n",
      "Recall: 0.23582089552238805, 0.2693156732891832, 0.89\n",
      "F1-score: 0.3726415094339623, 0.35777126099706746, 0.1943231441048035\n",
      "\n",
      "None: 2000\n",
      "\n",
      "annotation_only_7b\n",
      "Overall performance\n",
      "Macro precision: 0.42322214658497853\n",
      "Macro recall: 0.4596331529167352\n",
      "Micro precision: 0.40326530612244915\n",
      "Micro recall: 0.40326530612244915\n",
      "Accuracy: 0.40326530612244915\n",
      "\n",
      "Label-by-label (order: E-N-C)\n",
      "Precision: 0.6369230769230769, 0.5, 0.13274336283185842\n",
      "Recall: 0.6179104477611941, 0.01098901098901099, 0.75\n",
      "F1-score: 0.6272727272727273, 0.021505376344086023, 0.22556390977443613\n",
      "\n",
      "None: 0\n",
      "\n",
      "gpt-3_5_finetuned_on_gpt_4\n",
      "Overall performance\n",
      "Macro precision: 0.64248206571504\n",
      "Macro recall: 0.682062216390575\n",
      "Micro precision: 0.7338775510204082\n",
      "Micro recall: 0.7338775510204082\n",
      "Accuracy: 0.7338775510204082\n",
      "\n",
      "Label-by-label (order: E-N-C)\n",
      "Precision: 0.8126801152737753, 0.7638888888888888, 0.3508771929824561\n",
      "Recall: 0.8417910447761194, 0.6043956043956044, 0.6\n",
      "F1-score: 0.8269794721407625, 0.674846625766871, 0.44280442804428044\n",
      "\n",
      "None: 0\n",
      "\n",
      "gpt-3_5_finetuned_on_gpt_3_5\n",
      "Overall performance\n",
      "Macro precision: 0.5139210470445718\n",
      "Macro recall: 0.5852304412005901\n",
      "Micro precision: 0.5665306122448979\n",
      "Micro recall: 0.5665306122448979\n",
      "Accuracy: 0.5665306122448979\n",
      "\n",
      "Label-by-label (order: E-N-C)\n",
      "Precision: 0.7726574500768049, 0.5658536585365853, 0.2032520325203252\n",
      "Recall: 0.7507462686567165, 0.2549450549450549, 0.75\n",
      "F1-score: 0.761544284632854, 0.3515151515151515, 0.31982942430703626\n",
      "\n",
      "None: 0\n",
      "\n",
      "gpt-3_5_finetuned_on_70b\n",
      "Overall performance\n",
      "Macro precision: 0.5716637172985923\n",
      "Macro recall: 0.6479049805915478\n",
      "Micro precision: 0.6040816326530614\n",
      "Micro recall: 0.6040816326530614\n",
      "Accuracy: 0.6040816326530614\n",
      "\n",
      "Label-by-label (order: E-N-C)\n",
      "Precision: 0.8706365503080082, 0.6236842105263158, 0.2206703910614525\n",
      "Recall: 0.6328358208955224, 0.5208791208791209, 0.79\n",
      "F1-score: 0.7329299913569578, 0.5676646706586826, 0.34497816593886466\n",
      "\n",
      "None: 0\n",
      "\n",
      "13b_finetuned_on_gpt_4\n",
      "Overall performance\n",
      "Macro precision: 0.6265088283164998\n",
      "Macro recall: 0.6891411076485704\n",
      "Micro precision: 0.7118367346938772\n",
      "Micro recall: 0.7118367346938772\n",
      "Accuracy: 0.7118367346938772\n",
      "\n",
      "Label-by-label (order: E-N-C)\n",
      "Precision: 0.8561525129982669, 0.6811279826464208, 0.3422459893048128\n",
      "Recall: 0.7373134328358208, 0.6901098901098901, 0.64\n",
      "F1-score: 0.7923015236567762, 0.685589519650655, 0.445993031358885\n",
      "\n",
      "None: 0\n",
      "\n",
      "13b_finetuned_on_gpt_3_5\n",
      "Overall performance\n",
      "Macro precision: 0.5225576089961033\n",
      "Macro recall: 0.5704690831556504\n",
      "Micro precision: 0.6048979591836735\n",
      "Micro recall: 0.6048979591836735\n",
      "Accuracy: 0.6048979591836735\n",
      "\n",
      "Label-by-label (order: E-N-C)\n",
      "Precision: 0.7274074074074074, 0.5891238670694864, 0.2511415525114155\n",
      "Recall: 0.7328358208955223, 0.42857142857142855, 0.55\n",
      "F1-score: 0.7301115241635688, 0.4961832061068702, 0.3448275862068965\n",
      "\n",
      "None: 0\n",
      "\n",
      "13b_finetuned_on_70b\n",
      "Overall performance\n",
      "Macro precision: 0.5635757713077301\n",
      "Macro recall: 0.638288951717185\n",
      "Micro precision: 0.6331699346405231\n",
      "Micro recall: 0.6331699346405231\n",
      "Accuracy: 0.6331699346405231\n",
      "\n",
      "Label-by-label (order: E-N-C)\n",
      "Precision: 0.7989690721649485, 0.617948717948718, 0.27380952380952384\n",
      "Recall: 0.6940298507462687, 0.5308370044052864, 0.69\n",
      "F1-score: 0.7428115015974441, 0.571090047393365, 0.3920454545454546\n",
      "\n",
      "None: 1000\n",
      "\n",
      "7b_finetuned_on_gpt_4\n",
      "Overall performance\n",
      "Macro precision: 0.641249859472983\n",
      "Macro recall: 0.6996850910283746\n",
      "Micro precision: 0.7257142857142859\n",
      "Micro recall: 0.7257142857142859\n",
      "Accuracy: 0.7257142857142859\n",
      "\n",
      "Label-by-label (order: E-N-C)\n",
      "Precision: 0.8775137111517367, 0.6857707509881423, 0.36046511627906974\n",
      "Recall: 0.7164179104477612, 0.7626373626373626, 0.62\n",
      "F1-score: 0.7888249794576827, 0.7221644120707597, 0.45588235294117646\n",
      "\n",
      "None: 0\n",
      "\n",
      "7b_finetuned_on_gpt_3_5\n",
      "Overall performance\n",
      "Macro precision: 0.48330554398081893\n",
      "Macro recall: 0.5170231261276035\n",
      "Micro precision: 0.5551020408163263\n",
      "Micro recall: 0.5551020408163263\n",
      "Accuracy: 0.5551020408163263\n",
      "\n",
      "Label-by-label (order: E-N-C)\n",
      "Precision: 0.6657496561210454, 0.5776892430278885, 0.20647773279352227\n",
      "Recall: 0.7223880597014926, 0.31868131868131866, 0.51\n",
      "F1-score: 0.6929133858267716, 0.4107648725212465, 0.29394812680115273\n",
      "\n",
      "None: 0\n",
      "\n",
      "7b_finetuned_on_70b\n",
      "Overall performance\n",
      "Macro precision: 0.6020798326322611\n",
      "Macro recall: 0.6034497840467986\n",
      "Micro precision: 0.6767346938775507\n",
      "Micro recall: 0.6767346938775507\n",
      "Accuracy: 0.6767346938775507\n",
      "\n",
      "Label-by-label (order: E-N-C)\n",
      "Precision: 0.8066783831282952, 0.5913978494623656, 0.40816326530612246\n",
      "Recall: 0.6850746268656717, 0.7252747252747253, 0.4\n",
      "F1-score: 0.7409200968523002, 0.6515301085883514, 0.4040404040404041\n",
      "\n",
      "None: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in columns_eval:\n",
    "    try:\n",
    "\n",
    "        print(col)\n",
    "        print(evaluate_aggregated_mturks(original_df, col, sample=1000))\n",
    "        print()\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TH0ZoDYOwMq3",
    "eC4owfnjFsV2"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
