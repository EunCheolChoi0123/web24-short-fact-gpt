{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivLYNWTn2sLk"
   },
   "source": [
    "# Finetuning Llama-2 models\n",
    "- Initially fine-tuned via USC HPC clusters\n",
    "- With this script, it is possible to fine-tune on A100 or V100 with Google Colab Pro subscription or above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-AwYQ653Mqw"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3vwxcDMzylE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change directory\n",
    "os.chdir('/content/drive/MyDrive/Finetuning')\n",
    "\n",
    "# Verify that the current working directory has been changed\n",
    "print(os.getcwd())\n",
    "\n",
    "%cd /content/drive/MyDrive/Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0HQY-Gv_byW"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Efficient-Tuning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFOcKtXnBBtE"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install bitsandbytes>=0.39.0\n",
    "!pip install -r /content/drive/MyDrive/Finetuning/LLaMA-Efficient-Tuning/requirements.txt\n",
    "!pip install trl==0.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEjE-ht2IdjE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/content/drive/MyDrive/Finetuning/.cache/huggingface/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jI1TYqdW_oZI"
   },
   "outputs": [],
   "source": [
    "efficient_finetuning_folder = \"/content/drive/MyDrive/Finetuning/LLaMA-Efficient-Tuning\" #absolute path\n",
    "\n",
    "train_gpt_4 = \"train_gpt-4.json\"\n",
    "test = \"test.json\" # not the actual test set, but for a placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXTqhaOB_47s"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def add_json_file(efficient_finetuning_folder, json_file_name):\n",
    "    # Replace {username} with your actual username\n",
    "    data_info_file = f\"{efficient_finetuning_folder}/data/dataset_info.json\"\n",
    "\n",
    "    # Load the data_info.json file\n",
    "    with open(data_info_file, 'r') as f:\n",
    "        data_info = json.load(f)\n",
    "\n",
    "    # Create a new key by removing the .json extension from the file name\n",
    "    new_key = json_file_name.replace('.json', '')\n",
    "\n",
    "    # Add the new key to the data_info dictionary\n",
    "    data_info[new_key] = {\n",
    "        'file_name': json_file_name\n",
    "    }\n",
    "\n",
    "    # Save the updated data_info.json file\n",
    "    with open(data_info_file, 'w') as f:\n",
    "        json.dump(data_info, f, indent=4)\n",
    "\n",
    "    print(f'Added {new_key} to data_info.json')\n",
    "\n",
    "add_json_file(efficient_finetuning_folder, train_gpt_4)\n",
    "add_json_file(efficient_finetuning_folder, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9O9hWJsRpt7x"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token ### HUGGINGFACE API KEY ###\n",
    "%cd /content/drive/MyDrive/Finetuning/LLaMA-Efficient-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_-i1Wh7fChN"
   },
   "source": [
    "# FACT-GPT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30C-OeGJppHd"
   },
   "outputs": [],
   "source": [
    "def train_valid_llama(model_size, train_data):\n",
    "\n",
    "    command = f\"\"\"!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/Finetuning/LLaMA-Efficient-Tuning/src/train_bash.py \\\n",
    "        --stage sft \\\n",
    "        --model_name_or_path \"meta-llama/Llama-2-{model_size}-chat-hf\" \\\n",
    "        --do_train \\\n",
    "        --dataset \"{train_data}\" \\\n",
    "        --template \"default\" \\\n",
    "        --finetuning_type \"lora\" \\\n",
    "        --lora_target \"q_proj,v_proj\" \\\n",
    "        --output_dir \"/content/drive/MyDrive/Finetuning/train_valid_{model_size}_{train_data}\" \\\n",
    "        --overwrite_cache \\\n",
    "        --per_device_train_batch_size 4 \\\n",
    "        --gradient_accumulation_steps 4 \\\n",
    "        --lr_scheduler_type \"cosine\" \\\n",
    "        --logging_steps 1 \\\n",
    "        --save_steps 61 \\\n",
    "        --val_size 0.2 \\\n",
    "        --evaluation_strategy steps \\\n",
    "        --eval_steps 61 \\\n",
    "        --learning_rate \"5e-5\" \\\n",
    "        --num_train_epochs 3.0 \\\n",
    "        --plot_loss True \\\n",
    "        --fp16\"\"\"\n",
    "\n",
    "    print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sgcn4h_HhAZM"
   },
   "outputs": [],
   "source": [
    "def test_llama(model_size, train_data):\n",
    "\n",
    "    command = f\"\"\"!CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path 'meta-llama/Llama-2-{model_size}-chat-hf' \\\n",
    "    --do_predict \\\n",
    "    --dataset 'test' \\\n",
    "    --template 'default' \\\n",
    "    --finetuning_type 'lora' \\\n",
    "    --checkpoint_dir '/content/drive/MyDrive/Finetuning/train_valid_{model_size}_{train_data}' \\\n",
    "    --output_dir '/content/drive/MyDrive/Finetuning/train_valid_{model_size}_{train_data}/test-endpoint' \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --max_samples 10000 \\\n",
    "    --temperature 0.01 \\\n",
    "    --top_p 0.01 \\\n",
    "    --predict_with_generate\"\"\"\n",
    "\n",
    "    print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm8mA078jA-v"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbhjFzYKNIL3"
   },
   "outputs": [],
   "source": [
    "train_valid_llama('13b', 'train_gpt-4')\n",
    "train_valid_llama('7b', 'train_gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByUalVn_jEBP"
   },
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/Finetuning/LLaMA-Efficient-Tuning/src/train_bash.py         --stage sft         --model_name_or_path \"meta-llama/Llama-2-13b-chat-hf\"         --do_train         --dataset \"train_gpt-4\"         --template \"default\"         --finetuning_type \"lora\"         --lora_target \"q_proj,v_proj\"         --output_dir \"/content/drive/MyDrive/Finetuning/train_valid_13b_train_gpt-4\"         --overwrite_cache         --per_device_train_batch_size 4         --gradient_accumulation_steps 4         --lr_scheduler_type \"cosine\"         --logging_steps 1         --save_steps 61         --val_size 0.2         --evaluation_strategy steps         --eval_steps 61         --learning_rate \"5e-5\"         --num_train_epochs 3.0         --plot_loss True         --fp16\n",
    "!CUDA_VISIBLE_DEVICES=0 python /content/drive/MyDrive/Finetuning/LLaMA-Efficient-Tuning/src/train_bash.py         --stage sft         --model_name_or_path \"meta-llama/Llama-2-7b-chat-hf\"         --do_train         --dataset \"train_gpt-4\"         --template \"default\"         --finetuning_type \"lora\"         --lora_target \"q_proj,v_proj\"         --output_dir \"/content/drive/MyDrive/Finetuning/train_valid_7b_train_gpt-4\"         --overwrite_cache         --per_device_train_batch_size 4         --gradient_accumulation_steps 4         --lr_scheduler_type \"cosine\"         --logging_steps 1         --save_steps 61         --val_size 0.2         --evaluation_strategy steps         --eval_steps 61         --learning_rate \"5e-5\"         --num_train_epochs 3.0         --plot_loss True         --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIBKPRPHhMdr"
   },
   "outputs": [],
   "source": [
    "test_llama('13b', 'train_gpt-4')\n",
    "test_llama('7b', 'train_gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLH-47J6j_Yr"
   },
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python src/train_bash.py     --stage sft     --model_name_or_path 'meta-llama/Llama-2-13b-chat-hf'     --do_predict     --dataset 'test'     --template 'default'     --finetuning_type 'lora'     --checkpoint_dir '/content/drive/MyDrive/Finetuning/train_valid_13b_train_gpt-4'     --output_dir '/content/drive/MyDrive/Finetuning/train_valid_13b_train_gpt-4/test-endpoint'     --per_device_eval_batch_size 8     --max_samples 10000     --temperature 0.01     --top_p 0.01     --predict_with_generate\n",
    "!CUDA_VISIBLE_DEVICES=0 python src/train_bash.py     --stage sft     --model_name_or_path 'meta-llama/Llama-2-7b-chat-hf'     --do_predict     --dataset 'test'     --template 'default'     --finetuning_type 'lora'     --checkpoint_dir '/content/drive/MyDrive/Finetuning/train_valid_7b_train_gpt-4'     --output_dir '/content/drive/MyDrive/Finetuning/train_valid_7b_train_gpt-4/test-endpoint'     --per_device_eval_batch_size 8     --max_samples 10000     --temperature 0.01     --top_p 0.01     --predict_with_generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOj8mJESaVeo"
   },
   "source": [
    "# Put inference results into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_om-9ESzaaLa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/FACT-GPT dataset.csv')\n",
    "df['13b_finetuned_on_gpt_4'] = pd.read_json('/content/drive/MyDrive/Finetuning/train_valid_13b_train_gpt-4_balanced/test-endpoint/generated_predictions.jsonl', lines=True)['predict']\n",
    "df['13b_finetuned_on_gpt_3_5'] = pd.read_json('/content/drive/MyDrive/Finetuning/train_valid_13b_train_gpt-3_5_balanced/test-endpoint/generated_predictions.jsonl', lines=True)['predict']\n",
    "df['13b_finetuned_on_70b'] = pd.read_json('/content/drive/MyDrive/Finetuning/train_valid_13b_train_llama_2_70b_balanced/test-endpoint/generated_predictions.jsonl', lines=True)['predict']\n",
    "\n",
    "df['7b_finetuned_on_gpt_4'] = pd.read_json('/content/drive/MyDrive/Finetuning/train_valid_7b_train_gpt-4_balanced/test-endpoint/generated_predictions.jsonl', lines=True)['predict']\n",
    "df['7b_finetuned_on_gpt_3_5'] = pd.read_json('/content/drive/MyDrive/Finetuning/train_valid_7b_train_gpt-3_5_balanced/test-endpoint/generated_predictions.jsonl', lines=True)['predict']\n",
    "df['7b_finetuned_on_70b'] = pd.read_json('/content/drive/MyDrive/Finetuning/train_valid_7b_train_llama_2_70b_balanced/test-endpoint/generated_predictions.jsonl', lines=True)['predict']\n",
    "\n",
    "df.to_csv('/content/drive/MyDrive/FACT-GPT dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrVTXmemWmqK"
   },
   "source": [
    "# unassign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1e_ViyAd7Vq"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
